{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9b74b1-c9e8-4a07-a6e7-c956809c6a91",
   "metadata": {},
   "source": [
    "## 3 Modeling<a id='3_Modeling'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4469e550-3d94-4830-a4fc-464adfb12b98",
   "metadata": {},
   "source": [
    "## 3.1 Contents<a id='3.1_Contents'></a>\n",
    "* [3 Modeling](#3_Modeling)\n",
    "  * [3.1 Contents](#3.1_Contents)\n",
    "  * [3.2 Introduction](#3.2_Introduction)\n",
    "  * [3.3 Imports](#3.3_Imports)\n",
    "  * [3.4 Load The Model](#3.4_Load_The_Model)\n",
    "  * [3.5 Predicting Parameters](#3.5_Predicting_Parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5056c961-a5dd-4e47-ad3a-5f628eadba5c",
   "metadata": {},
   "source": [
    "## 3.2 Introduction<a id='3.2_Introduction'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f057338-0337-4d0d-afc8-8a967e4e6ee3",
   "metadata": {},
   "source": [
    "We will finally be testing out our model in recreating an audio sample with a synthesizer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3340c774-4da0-4ffc-9909-9e3624a281a9",
   "metadata": {},
   "source": [
    "## 3.3 Imports<a id='3.3_Imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026a6272",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/notis/.local/lib/python3.10/site-packages/torch_lr_finder/lr_finder.py:5: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math, random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from IPython.display import Audio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from torch_lr_finder import LRFinder\n",
    "import dawdreamer as daw\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2698944-500b-428e-94a0-fa28569ed61f",
   "metadata": {},
   "source": [
    "First step will be to import the appropriate libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedbc5e-6a29-417a-9a83-0dded60c4591",
   "metadata": {},
   "source": [
    "## 3.4 Load The Model<a id='3.4_Load_The_Model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e03d3e-bdca-4e1b-854f-f758a0fa3592",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(AudioLSTMClassifier, self).__init__()\n",
    "        \n",
    "        # Initialize the LSTM layers\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # The regressor to output the desired number of parameters\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 128),\n",
    "            nn.Hardswish(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Combine batch size and channels, and treat freq_bins as seq_length for the LSTM\n",
    "        batch_size, channels, freq_bins, time_steps = x.size()\n",
    "        x = x.reshape(batch_size, channels * freq_bins, time_steps)  # Use reshape instead of view\n",
    "        x = x.transpose(1, 2)  # LSTM expects [batch_size, seq_length, features]\n",
    "        \n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate the LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Take the output of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.regressor(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "\n",
    "    def sanitize_data(inputs, labels):\n",
    "        # Replace NaNs with a specified value or handle them accordingly\n",
    "        inputs[torch.isnan(inputs)] = 0\n",
    "        labels[torch.isnan(labels)] = 0\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152b09b-33bc-489f-a7bd-0aefd953f42e",
   "metadata": {},
   "source": [
    "To load the model we must redefine the class to created the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd73bcc9-e5d5-4493-8e1d-f5b9f1c93f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "myModel = torch.load('../models/my_model_2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179677c3-ad44-429e-a417-0867714ca9b2",
   "metadata": {},
   "source": [
    "Next we load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cace2c0a-47a7-48de-a8a6-eab636eebe8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioLSTMClassifier(\n",
       "  (lstm): LSTM(128, 128, num_layers=5, batch_first=True)\n",
       "  (regressor): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=318, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myModel.load_state_dict(torch.load('../models/model_2_weights.pth'))\n",
    "myModel.eval()  # Set the model to inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648ad8c-a63b-4878-bf51-7fa65b5bba66",
   "metadata": {},
   "source": [
    "We then load the model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dbee0e-832b-4eda-939f-b12d7b3d8c04",
   "metadata": {},
   "source": [
    "## 3.5 Load Audio Processing<a id='3.5_Load_Audio_Processing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "745daf68-1338-44cd-873f-ff6bce88ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil():\n",
    "    # ----------------------------\n",
    "    # Load an audio file. Return the signal as a tensor and the sample rate\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Convert the given audio to the desired number of channels\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def rechannel(aud, new_channel):\n",
    "        sig, sr = aud\n",
    "        \n",
    "        if (sig.shape[0] == new_channel):\n",
    "            # Nothing to do\n",
    "            return aud\n",
    "    \n",
    "        if (new_channel == 1):\n",
    "            # Convert from stereo to mono by selecting only the first channel\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            # Convert from mono to stereo by duplicating the first channel\n",
    "            resig = torch.cat([sig, sig])\n",
    "        \n",
    "        return ((resig, sr))\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Since Resample applies to a single channel, we resample one channel at a time\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "    \n",
    "        if (sr == newsr):\n",
    "            # Nothing to do\n",
    "            return aud\n",
    "    \n",
    "        num_channels = sig.shape[0]\n",
    "        # Resample first channel\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1,:])\n",
    "        if (num_channels > 1):\n",
    "            # Resample the second channel and merge both channels\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:,:])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "    \n",
    "            return ((resig, newsr))\n",
    "            \n",
    "    # ----------------------------\n",
    "    # Pad (or truncate) the signal to a fixed length 'max_ms' in milliseconds\n",
    "    # ----------------------------\n",
    "    \n",
    "    @staticmethod\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "        \n",
    "        if (sig_len > max_len):\n",
    "            # Truncate the signal to the given length\n",
    "            sig = sig[:,:max_len]\n",
    "        \n",
    "        elif (sig_len < max_len):\n",
    "            # Length of padding to add at the beginning and end of the signal\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "            \n",
    "            # Pad with 0s\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "            \n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "      \n",
    "        return (sig, sr)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Generate a Spectrogram\n",
    "    # ----------------------------\n",
    "    @staticmethod\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig,sr = aud\n",
    "        top_db = 80\n",
    "        \n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n",
    "        spec = transforms.MelSpectrogram(sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "        \n",
    "        # Convert to decibels\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        return (spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f667bf-eec5-4af0-a69a-03533252bef2",
   "metadata": {},
   "source": [
    "We also need to load the `AudioUtil` class that was introduced in the pre-processing and training to process the audio samples that we will be testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76beed7e-0bf2-4cc2-b852-118f8da82638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class SoundTest(Dataset):\n",
    "    def __init__(self, test_audio, data_path):\n",
    "        self.test_audio = str(test_audio)\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 4000\n",
    "        self.sr = 44100\n",
    "        self.channel = 2 \n",
    "    \n",
    "    # ----------------------------\n",
    "    # Get the test item in dataset\n",
    "    # ----------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with\n",
    "        # the relative path\n",
    "        audio_file = self.data_path + '/' + self.test_audio\n",
    "        \n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        # Some sounds have a higher sample rate, or fewer channels compared to the\n",
    "        # majority. So make all sounds have the same number of channels and same \n",
    "        # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
    "        # result in arrays of different lengths, even though the sound duration is\n",
    "        # the same.\n",
    "        reaud = AudioUtil.resample(aud, self.sr)\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "        # shorten = AudioUtil.pad_trunc(rechan, 1000)\n",
    "        \n",
    "        sgram = AudioUtil.spectro_gram(rechan, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        \n",
    "        return sgram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204003c9-4cd3-49fd-8d4b-43a5765be4f8",
   "metadata": {},
   "source": [
    "We slightly modified the `SoundDS` class to create the `SoundTest` that way we only return the spectrogram for testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06de3c5-740d-4d53-8960-1b7474a0be60",
   "metadata": {},
   "source": [
    "## 3.5 Predicting Parameters<a id='3.5_Predicting_Parameters'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611e4c40-6a57-4506-8390-105b7ec4c1b4",
   "metadata": {},
   "source": [
    "We will now grab an audio sample and make predictions on its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3f78b7e-4a02-44bd-8926-f459a594609f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item shape (spectrogram): torch.Size([64, 431])\n",
      "First few target values: tensor([ 26.8532,   0.0973,  -9.0143, -12.2877, -17.3538])\n"
     ]
    }
   ],
   "source": [
    "# Set the audio_path\n",
    "audio_path = \"../data/raw/data_audio\"\n",
    "\n",
    "# Set the test_sample_data\n",
    "test_sample_data = \"BA - Basic Bass.mp3\"\n",
    "\n",
    "# Create an instance of your test sample\n",
    "test_sample = SoundTest(test_sample_data, audio_path)\n",
    "\n",
    "input_data = SoundTest(test_sample_data, audio_path)\n",
    "\n",
    "# Example item check in the dataset\n",
    "def test_getitem(dataset, index):\n",
    "    item, targets = dataset[index]\n",
    "    print(f\"Item shape (spectrogram): {item.shape}\")\n",
    "    print(f\"First few target values: {targets[0][:5]}\")  # Print the elements of the targets\n",
    "\n",
    "# Test __getitem__ on the first item\n",
    "test_getitem(input_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7b8141-8370-4c17-98c8-84a0d342f7c7",
   "metadata": {},
   "source": [
    "First we load in the audio sample that we will be testing. We retrieved the file `\"BA - Basic Bass.mp3\"`. We make sure the `SoundTest` class is working with the `test_getitem` function. The output shows the tensor values of the spectrogram created from the audio sample we selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3e66ec-91d9-409b-a29a-cccdbddbc39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SoundTest(test_sample_data, audio_path)\n",
    "test_sample = test_dataset[0]  # Retrieve the spectrogram from the dataset\n",
    "\n",
    "# Convert to a batch of size 1\n",
    "test_sample = test_sample.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "myModel.to(device)\n",
    "test_sample = test_sample.to(device)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    output = myModel(test_sample)\n",
    "    prediction = output.cpu().numpy()  # Move back to CPU and convert to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeebcac4-5045-4faf-8fb7-ef05eb30c024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.662135  , 0.58175725, 0.49345618, 0.40133193, 0.5048777 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6581706-62b1-4de5-89de-eae04ea72c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1148f6b6-1eb1-4613-9aec-8bdb6cd7f25f",
   "metadata": {},
   "source": [
    "We return the expected number out parameters for the audio test sample. We will save this prediction and load it into Serum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47dc3853-f742-40ff-89b7-c8b55a48a2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_csv('../data/interim/combined_data.csv',index_col=0)\n",
    "\n",
    "combined_data = combined_data.drop(columns=['Preset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddfb7138-84d1-48d8-8675-6c1a3ef44489",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = combined_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "471569e3-4f56-45bd-aa37-e67d6abe7a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = combined_data.columns.tolist()\n",
    "\n",
    "prediction_df = pd.DataFrame({\n",
    "    'Parameters': column_names,\n",
    "    'Values': prediction[0]\n",
    "})\n",
    "\n",
    "# Save the new dataframe to a CSV file\n",
    "prediction_df.to_csv('../data/interim/prediction.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
